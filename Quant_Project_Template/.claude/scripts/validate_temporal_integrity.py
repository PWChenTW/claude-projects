#!/usr/bin/env python3
"""
æ™‚åºæ•¸æ“šå®Œæ•´æ€§é©—è­‰å™¨
ç”¨æ–¼æª¢æ¸¬é‡åŒ–äº¤æ˜“æ¨¡å‹ä¸­çš„å¸¸è¦‹æ™‚åºå•é¡Œ
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import warnings
from datetime import datetime, timedelta

class TemporalIntegrityValidator:
    """æ™‚åºæ•¸æ“šå®Œæ•´æ€§é©—è­‰å™¨"""
    
    def __init__(self, verbose: bool = True):
        self.verbose = verbose
        self.issues = []
        
    def validate_train_test_split(self, 
                                 train_data: pd.DataFrame, 
                                 val_data: pd.DataFrame = None,
                                 test_data: pd.DataFrame = None) -> Dict[str, Any]:
        """
        é©—è­‰è¨“ç·´/é©—è­‰/æ¸¬è©¦é›†åˆ†å‰²æ˜¯å¦æ­£ç¢º
        
        Returns:
            Dict containing validation results
        """
        results = {
            'valid': True,
            'issues': [],
            'warnings': []
        }
        
        # æª¢æŸ¥æ™‚é–“ç´¢å¼•
        if not isinstance(train_data.index, pd.DatetimeIndex):
            results['issues'].append("è¨“ç·´é›†æ²’æœ‰ä½¿ç”¨ DatetimeIndex")
            results['valid'] = False
            
        # æª¢æŸ¥æ™‚åºé †åº
        train_end = train_data.index.max()
        
        if val_data is not None:
            val_start = val_data.index.min()
            val_end = val_data.index.max()
            
            if val_start <= train_end:
                results['issues'].append(
                    f"é©—è­‰é›†é–‹å§‹æ™‚é–“ ({val_start}) æ—©æ–¼æˆ–ç­‰æ–¼è¨“ç·´é›†çµæŸæ™‚é–“ ({train_end})"
                )
                results['valid'] = False
                
        if test_data is not None:
            test_start = test_data.index.min()
            
            if val_data is not None:
                if test_start <= val_end:
                    results['issues'].append(
                        f"æ¸¬è©¦é›†é–‹å§‹æ™‚é–“ ({test_start}) æ—©æ–¼æˆ–ç­‰æ–¼é©—è­‰é›†çµæŸæ™‚é–“ ({val_end})"
                    )
                    results['valid'] = False
            else:
                if test_start <= train_end:
                    results['issues'].append(
                        f"æ¸¬è©¦é›†é–‹å§‹æ™‚é–“ ({test_start}) æ—©æ–¼æˆ–ç­‰æ–¼è¨“ç·´é›†çµæŸæ™‚é–“ ({train_end})"
                    )
                    results['valid'] = False
                    
        # æª¢æŸ¥æ•¸æ“šé‡ç–Š
        if val_data is not None:
            overlap = set(train_data.index) & set(val_data.index)
            if overlap:
                results['issues'].append(
                    f"è¨“ç·´é›†å’Œé©—è­‰é›†æœ‰ {len(overlap)} å€‹é‡ç–Šçš„æ™‚é–“é»"
                )
                results['valid'] = False
                
        if test_data is not None:
            if val_data is not None:
                overlap = set(val_data.index) & set(test_data.index)
                if overlap:
                    results['issues'].append(
                        f"é©—è­‰é›†å’Œæ¸¬è©¦é›†æœ‰ {len(overlap)} å€‹é‡ç–Šçš„æ™‚é–“é»"
                    )
                    results['valid'] = False
                    
            overlap = set(train_data.index) & set(test_data.index)
            if overlap:
                results['issues'].append(
                    f"è¨“ç·´é›†å’Œæ¸¬è©¦é›†æœ‰ {len(overlap)} å€‹é‡ç–Šçš„æ™‚é–“é»"
                )
                results['valid'] = False
                
        # å»ºè­°çš„åˆ†å‰²æ¯”ä¾‹
        total_size = len(train_data)
        if val_data is not None:
            total_size += len(val_data)
        if test_data is not None:
            total_size += len(test_data)
            
        train_ratio = len(train_data) / total_size
        
        if train_ratio < 0.6:
            results['warnings'].append(
                f"è¨“ç·´é›†æ¯”ä¾‹ ({train_ratio:.1%}) å¯èƒ½å¤ªå°ï¼Œå»ºè­°è‡³å°‘ 60%"
            )
        elif train_ratio > 0.8:
            results['warnings'].append(
                f"è¨“ç·´é›†æ¯”ä¾‹ ({train_ratio:.1%}) å¯èƒ½å¤ªå¤§ï¼Œå»ºè­°ä¿ç•™è¶³å¤ çš„é©—è­‰/æ¸¬è©¦æ•¸æ“š"
            )
            
        if self.verbose:
            self._print_results("è¨“ç·´/æ¸¬è©¦é›†åˆ†å‰²é©—è­‰", results)
            
        return results
        
    def check_look_ahead_bias(self, 
                             data: pd.DataFrame,
                             feature_columns: List[str],
                             target_column: str = None) -> Dict[str, Any]:
        """
        æª¢æ¸¬æ½›åœ¨çš„å‰è¦–åå·®
        
        Args:
            data: åŒ…å«ç‰¹å¾µçš„æ•¸æ“š
            feature_columns: ç‰¹å¾µåˆ—å
            target_column: ç›®æ¨™åˆ—åï¼ˆå¯é¸ï¼‰
            
        Returns:
            Dict containing potential look-ahead bias issues
        """
        results = {
            'valid': True,
            'suspicious_features': [],
            'warnings': []
        }
        
        for col in feature_columns:
            if col not in data.columns:
                continue
                
            # æª¢æŸ¥æ˜¯å¦ä½¿ç”¨äº†æœªä¾†æ•¸æ“šçš„è·¡è±¡
            # 1. æª¢æŸ¥å®Œç¾é æ¸¬
            if target_column and target_column in data.columns:
                correlation = data[col].corr(data[target_column])
                if abs(correlation) > 0.99:
                    results['suspicious_features'].append({
                        'feature': col,
                        'issue': f'èˆ‡ç›®æ¨™è®Šé‡ç›¸é—œæ€§éé«˜ ({correlation:.3f})',
                        'severity': 'HIGH'
                    })
                    results['valid'] = False
                    
            # 2. æª¢æŸ¥ä¸åˆç†çš„å¹³æ»‘åº¦
            if len(data) > 100:
                # è¨ˆç®—è‡ªç›¸é—œ
                autocorr = data[col].autocorr(1)
                if autocorr > 0.999:
                    results['warnings'].append({
                        'feature': col,
                        'issue': f'è‡ªç›¸é—œéé«˜ ({autocorr:.3f})ï¼Œå¯èƒ½ä½¿ç”¨äº†ä¸­å¿ƒåŒ–çª—å£',
                        'severity': 'MEDIUM'
                    })
                    
            # 3. æª¢æŸ¥æœªä¾†å€¼æ´©æ¼
            # é€šéæª¢æŸ¥ç‰¹å¾µå€¼æ˜¯å¦"é çŸ¥"äº†æœªä¾†çš„è®ŠåŒ–
            if len(data) > 20:
                future_corr = data[col].shift(-1).corr(data[col])
                if future_corr > 0.95:
                    results['warnings'].append({
                        'feature': col,
                        'issue': f'èˆ‡æœªä¾†å€¼ç›¸é—œæ€§éé«˜ ({future_corr:.3f})',
                        'severity': 'MEDIUM'
                    })
                    
        if self.verbose:
            self._print_results("å‰è¦–åå·®æª¢æ¸¬", results)
            
        return results
        
    def validate_feature_calculation(self, 
                                    data: pd.DataFrame,
                                    feature_func: callable,
                                    window_size: int = None) -> Dict[str, Any]:
        """
        é©—è­‰ç‰¹å¾µè¨ˆç®—æ˜¯å¦ç¬¦åˆæ™‚åºè¦æ±‚
        
        Args:
            data: åŸå§‹æ•¸æ“š
            feature_func: ç‰¹å¾µè¨ˆç®—å‡½æ•¸
            window_size: çª—å£å¤§å°ï¼ˆå¦‚é©ç”¨ï¼‰
            
        Returns:
            Dict containing validation results
        """
        results = {
            'valid': True,
            'issues': []
        }
        
        # æ¸¬è©¦ç‰¹å¾µè¨ˆç®—åœ¨ä¸åŒæ™‚é–“é»çš„ä¸€è‡´æ€§
        test_points = [len(data)//4, len(data)//2, 3*len(data)//4]
        
        for point in test_points:
            # ä½¿ç”¨æˆªæ–·çš„æ•¸æ“šè¨ˆç®—ç‰¹å¾µ
            truncated_data = data.iloc[:point]
            try:
                feature_truncated = feature_func(truncated_data)
                
                # ä½¿ç”¨å®Œæ•´æ•¸æ“šè¨ˆç®—ï¼Œç„¶å¾Œæˆªæ–·
                feature_full = feature_func(data)
                if isinstance(feature_full, pd.Series):
                    feature_full_truncated = feature_full.iloc[:point]
                else:
                    feature_full_truncated = feature_full[:point]
                
                # æ¯”è¼ƒçµæœ
                if not np.allclose(feature_truncated, feature_full_truncated, rtol=1e-5):
                    results['issues'].append(
                        f"ç‰¹å¾µè¨ˆç®—åœ¨æ™‚é–“é» {point} ä¸ä¸€è‡´ï¼Œå¯èƒ½ä½¿ç”¨äº†æœªä¾†æ•¸æ“š"
                    )
                    results['valid'] = False
                    
            except Exception as e:
                results['issues'].append(f"ç‰¹å¾µè¨ˆç®—å¤±æ•—: {str(e)}")
                results['valid'] = False
                
        if self.verbose:
            self._print_results("ç‰¹å¾µè¨ˆç®—é©—è­‰", results)
            
        return results
        
    def check_data_leakage(self, 
                          X_train: pd.DataFrame,
                          X_test: pd.DataFrame,
                          threshold: float = 0.001) -> Dict[str, Any]:
        """
        æª¢æ¸¬è¨“ç·´é›†å’Œæ¸¬è©¦é›†ä¹‹é–“çš„æ•¸æ“šæ´©æ¼
        
        Args:
            X_train: è¨“ç·´ç‰¹å¾µ
            X_test: æ¸¬è©¦ç‰¹å¾µ
            threshold: ç›¸ä¼¼åº¦é–¾å€¼
            
        Returns:
            Dict containing leakage detection results
        """
        results = {
            'valid': True,
            'duplicate_rows': 0,
            'similar_rows': 0,
            'suspicious_columns': []
        }
        
        # æª¢æŸ¥å®Œå…¨é‡è¤‡çš„è¡Œ
        train_set = set(map(tuple, X_train.values))
        test_set = set(map(tuple, X_test.values))
        duplicates = train_set & test_set
        
        if duplicates:
            results['duplicate_rows'] = len(duplicates)
            results['valid'] = False
            results['issues'] = [f"ç™¼ç¾ {len(duplicates)} å€‹å®Œå…¨é‡è¤‡çš„æ•¸æ“šè¡Œ"]
            
        # æª¢æŸ¥æ¯åˆ—çš„çµ±è¨ˆç‰¹æ€§
        for col in X_train.columns:
            if col not in X_test.columns:
                continue
                
            # æª¢æŸ¥æ˜¯å¦æœ‰å®Œå…¨ç›¸åŒçš„å€¼åˆ†å¸ƒ
            train_unique = X_train[col].nunique()
            test_unique = X_test[col].nunique()
            
            if train_unique == 1 and test_unique == 1:
                if X_train[col].iloc[0] == X_test[col].iloc[0]:
                    results['suspicious_columns'].append({
                        'column': col,
                        'issue': 'è¨“ç·´é›†å’Œæ¸¬è©¦é›†çš„å€¼å®Œå…¨ç›¸åŒ'
                    })
                    
        if self.verbose:
            self._print_results("æ•¸æ“šæ´©æ¼æª¢æ¸¬", results)
            
        return results
        
    def validate_cross_validation(self, 
                                cv_splits: List[Tuple[np.ndarray, np.ndarray]],
                                time_index: pd.DatetimeIndex = None) -> Dict[str, Any]:
        """
        é©—è­‰äº¤å‰é©—è­‰æ˜¯å¦ç¬¦åˆæ™‚åºè¦æ±‚
        
        Args:
            cv_splits: äº¤å‰é©—è­‰åˆ†å‰² [(train_idx, test_idx), ...]
            time_index: æ™‚é–“ç´¢å¼•
            
        Returns:
            Dict containing CV validation results
        """
        results = {
            'valid': True,
            'issues': [],
            'warnings': []
        }
        
        for i, (train_idx, test_idx) in enumerate(cv_splits):
            # æª¢æŸ¥æ™‚åºé †åº
            if time_index is not None:
                train_times = time_index[train_idx]
                test_times = time_index[test_idx]
                
                if train_times.max() >= test_times.min():
                    results['issues'].append(
                        f"ç¬¬ {i+1} æŠ˜: è¨“ç·´é›†åŒ…å«æ¸¬è©¦é›†ä¹‹å¾Œçš„æ•¸æ“š"
                    )
                    results['valid'] = False
                    
            # æª¢æŸ¥æ•¸æ“šé‡ç–Š
            overlap = set(train_idx) & set(test_idx)
            if overlap:
                results['issues'].append(
                    f"ç¬¬ {i+1} æŠ˜: è¨“ç·´é›†å’Œæ¸¬è©¦é›†æœ‰ {len(overlap)} å€‹é‡ç–Šæ¨£æœ¬"
                )
                results['valid'] = False
                
        # æª¢æŸ¥æ˜¯å¦ç‚ºæ™‚åºäº¤å‰é©—è­‰ï¼ˆå¾Œé¢çš„æŠ˜æ‡‰è©²ä½¿ç”¨æ›´å¤šçš„è¨“ç·´æ•¸æ“šï¼‰
        train_sizes = [len(train_idx) for train_idx, _ in cv_splits]
        if not all(train_sizes[i] <= train_sizes[i+1] for i in range(len(train_sizes)-1)):
            results['warnings'].append(
                "äº¤å‰é©—è­‰æŠ˜çš„è¨“ç·´é›†å¤§å°ä¸æ˜¯éå¢çš„ï¼Œå¯èƒ½ä¸æ˜¯æ™‚åºäº¤å‰é©—è­‰"
            )
            
        if self.verbose:
            self._print_results("äº¤å‰é©—è­‰æª¢æŸ¥", results)
            
        return results
        
    def _print_results(self, title: str, results: Dict[str, Any]):
        """æ‰“å°é©—è­‰çµæœ"""
        print(f"\n{'='*50}")
        print(f"{title}")
        print('='*50)
        
        if results['valid']:
            print("âœ… é€šéé©—è­‰")
        else:
            print("âŒ ç™¼ç¾å•é¡Œ:")
            if 'issues' in results:
                for issue in results['issues']:
                    print(f"  - {issue}")
                    
        if 'warnings' in results and results['warnings']:
            print("\nâš ï¸ è­¦å‘Š:")
            for warning in results['warnings']:
                if isinstance(warning, dict):
                    print(f"  - {warning['feature']}: {warning['issue']}")
                else:
                    print(f"  - {warning}")
                    
        if 'suspicious_features' in results and results['suspicious_features']:
            print("\nğŸ” å¯ç–‘ç‰¹å¾µ:")
            for feat in results['suspicious_features']:
                print(f"  - {feat['feature']}: {feat['issue']} [{feat['severity']}]")
                
        if 'suspicious_columns' in results and results['suspicious_columns']:
            print("\nğŸ” å¯ç–‘åˆ—:")
            for col in results['suspicious_columns']:
                print(f"  - {col['column']}: {col['issue']}")
                

def example_usage():
    """ä½¿ç”¨ç¯„ä¾‹"""
    # å‰µå»ºç¤ºä¾‹æ•¸æ“š
    dates = pd.date_range('2020-01-01', '2023-12-31', freq='D')
    n = len(dates)
    
    # å‰µå»ºå¸¶æœ‰æ™‚åºç‰¹æ€§çš„æ•¸æ“š
    data = pd.DataFrame({
        'price': 100 + np.cumsum(np.random.randn(n) * 0.5),
        'volume': np.random.exponential(1000, n),
        'returns': np.random.randn(n) * 0.01
    }, index=dates)
    
    # æ·»åŠ ä¸€äº›ç‰¹å¾µ
    data['sma_20'] = data['price'].rolling(20).mean()  # æ­£ç¢º
    data['future_leak'] = data['price'].shift(-1)  # éŒ¯èª¤ï¼šæœªä¾†æ•¸æ“šæ´©æ¼
    
    # åˆ†å‰²æ•¸æ“š
    train_end = '2022-12-31'
    val_end = '2023-06-30'
    
    train_data = data[data.index <= train_end]
    val_data = data[(data.index > train_end) & (data.index <= val_end)]
    test_data = data[data.index > val_end]
    
    # å‰µå»ºé©—è­‰å™¨
    validator = TemporalIntegrityValidator(verbose=True)
    
    # 1. é©—è­‰è¨“ç·´/æ¸¬è©¦åˆ†å‰²
    split_results = validator.validate_train_test_split(train_data, val_data, test_data)
    
    # 2. æª¢æŸ¥å‰è¦–åå·®
    feature_cols = ['sma_20', 'future_leak', 'volume']
    bias_results = validator.check_look_ahead_bias(
        train_data, 
        feature_cols, 
        target_column='returns'
    )
    
    # 3. æª¢æŸ¥æ•¸æ“šæ´©æ¼
    X_train = train_data[['sma_20', 'volume']]
    X_test = test_data[['sma_20', 'volume']]
    leakage_results = validator.check_data_leakage(X_train, X_test)
    
    # ç¸½çµ
    print("\n" + "="*50)
    print("é©—è­‰ç¸½çµ")
    print("="*50)
    
    all_valid = (split_results['valid'] and 
                bias_results['valid'] and 
                leakage_results['valid'])
    
    if all_valid:
        print("âœ… æ‰€æœ‰é©—è­‰é€šéï¼æ•¸æ“šè™•ç†ç¬¦åˆæ™‚åºè¦æ±‚ã€‚")
    else:
        print("âŒ ç™¼ç¾æ™‚åºæ•¸æ“šå•é¡Œï¼Œè«‹ä¿®æ­£å¾Œå†é€²è¡Œæ¨¡å‹è¨“ç·´ã€‚")
        

if __name__ == "__main__":
    example_usage()